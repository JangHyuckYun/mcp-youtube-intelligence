"""Data quality verification - Round 2 with working videos."""
import re, sys
sys.path.insert(0, "src")

from mcp_youtube_intelligence.core.transcript import fetch_transcript, clean_transcript, _NOISE_RE
from mcp_youtube_intelligence.core.summarizer import extractive_summary
from mcp_youtube_intelligence.core.segmenter import segment_topics
from mcp_youtube_intelligence.core.entities import extract_entities
from mcp_youtube_intelligence.core.comments import fetch_comments, summarize_comments, _is_noise, _analyze_sentiment

VIDEOS = {
    "A": {"id": "PkZNo7MFNFg", "name": "Learn Python (ì˜ì–´ ê¸´ ì˜ìƒ ~4.5h)", "lang": "en"},
    "B": {"id": "aircAruvnKk", "name": "í•œêµ­ì–´ ì˜ìƒ (ko manual)", "lang": "ko"},
    "C": {"id": "dQw4w9WgXcQ", "name": "Rick Astley - ì§§ì€ ì˜ìƒ", "lang": "en"},
}

for label, info in VIDEOS.items():
    vid = info["id"]
    print(f"\n{'='*70}")
    print(f"ì˜ìƒ {label}: {info['name']} (ID: {vid})")
    print(f"{'='*70}")
    
    tr = fetch_transcript(vid)
    raw = tr.get("best", "") or ""
    lang = tr.get("lang", "unknown")
    print(f"ì–¸ì–´: {lang}, ì›ë³¸: {len(raw)} chars")
    if not raw:
        print("âŒ ìë§‰ ì—†ìŒ"); continue
    
    # === 1. Transcript Cleaning ===
    cleaned = clean_transcript(raw)
    removed = len(raw) - len(cleaned)
    rate = removed / len(raw) * 100
    noise_matches = _NOISE_RE.findall(raw)
    
    print(f"\n--- 1. ìë§‰ ì •ì œ ---")
    print(f"ì œê±°ìœ¨: {rate:.1f}% ({removed} chars)")
    print(f"ë…¸ì´ì¦ˆ ë§¤ì¹˜: {len(noise_matches)}")
    if noise_matches:
        print(f"ë…¸ì´ì¦ˆ ìƒ˜í”Œ: {list(set(noise_matches))[:10]}")
    print(f"ì›ë³¸ ì²˜ìŒ 150ì: {raw[:150]}")
    print(f"ì •ì œ í›„ 150ì: {cleaned[:150]}")
    
    # === 2. Extractive Summary ===
    summary = extractive_summary(cleaned, max_sentences=5, max_chars=1000)
    s_ratio = len(summary) / len(cleaned) * 100
    sents = [s.strip() for s in re.split(r'[.!?ã€‚]\s+', summary) if s.strip()]
    incomplete = [s for s in sents if len(s) < 10]
    
    print(f"\n--- 2. ì¶”ì¶œì‹ ìš”ì•½ ---")
    print(f"ìš”ì•½: {len(summary)} chars ({s_ratio:.1f}% of cleaned)")
    print(f"ë¬¸ì¥ ìˆ˜: {len(sents)}, ë¶ˆì™„ì „: {len(incomplete)}")
    print(f"ìš”ì•½ ë‚´ìš©: {summary[:300]}")
    
    # === 3. Topic Segmentation ===
    segments = segment_topics(cleaned)
    print(f"\n--- 3. í† í”½ ì„¸ê·¸ë©˜í…Œì´ì…˜ ---")
    print(f"ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}")
    for seg in segments[:8]:
        print(f"  [{seg['segment']}] {seg['char_count']}ch: {seg['text'][:70]}...")
    
    # === 4. Entity Extraction ===
    entities = extract_entities(cleaned)
    print(f"\n--- 4. ì—”í‹°í‹° ì¶”ì¶œ ---")
    print(f"ì¶”ì¶œ: {len(entities)}")
    for e in entities[:10]:
        print(f"  {e['type']:10} {e['name']:20} kw='{e['keyword']}' cnt={e['count']}")
    
    # === 5. Comments ===
    print(f"\n--- 5. ëŒ“ê¸€ ---")
    comments = fetch_comments(vid, max_comments=15)
    print(f"ëŒ“ê¸€: {len(comments)}")
    if comments:
        cs = summarize_comments(comments)
        print(f"ê°ì„±: {cs['sentiment_ratio']}")
        for i, c in enumerate(comments[:10]):
            print(f"  {i+1}. [{c['sentiment']:8}] {c['text'][:60]}")

# === Edge Case Tests ===
print(f"\n{'='*70}")
print("ì—£ì§€ ì¼€ì´ìŠ¤ + ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
print(f"{'='*70}")

# Noise cleaning preserves meaning
tests = [
    ("[ìŒì•…] ìŒì•… ì‚°ì—…ì€ ì„±ì¥í•©ë‹ˆë‹¤", "ìŒì•… ì‚°ì—…"),
    ("[Music] The music industry grows", "music industry"),
    ("1:23 ì‹œê°„ì— 2:34 ì´ì•¼ê¸°", "ì‹œê°„ì— ì´ì•¼ê¸°"),
    ("ì•„ ì–´ ìŒ ê·¸ë˜ì„œ ê²°ë¡ ", "ê·¸ë˜ì„œ ê²°ë¡ "),
    ("[ë°•ìˆ˜] ë°•ìˆ˜ë¥¼ ë³´ëƒ…ë‹ˆë‹¤", "ë°•ìˆ˜ë¥¼ ë³´ëƒ…ë‹ˆë‹¤"),
]
print("\n[ë…¸ì´ì¦ˆ ì œê±° ì˜ë¯¸ ë³´ì¡´ í…ŒìŠ¤íŠ¸]")
for raw_t, must in tests:
    c = clean_transcript(raw_t)
    ok = must in c
    print(f"  {'âœ…' if ok else 'âŒ'} '{raw_t}' â†’ '{c}' (must: '{must}')")

# Longest match
print("\n[Longest-match í…ŒìŠ¤íŠ¸]")
t1 = "ì‚¼ì„±ì „ìê°€ ì¢‹ë‹¤. ì‚¼ì„±ì˜ ë¯¸ë˜."
e1 = extract_entities(t1)
for e in e1:
    print(f"  {e['keyword']}: count={e['count']}")
# ì‚¼ì„±ì „ì should match first, then ì‚¼ì„± in second sentence
has_samsung = any(e['keyword'] == 'ì‚¼ì„±ì „ì' for e in e1)
has_samsung2 = any(e['keyword'] == 'ì‚¼ì„±' for e in e1)
print(f"  ì‚¼ì„±ì „ì ë§¤ì¹˜: {has_samsung}, ì‚¼ì„±(ë³„ë„) ë§¤ì¹˜: {has_samsung2}")

# English segmenter
print("\n[ì˜ì–´ ì„¸ê·¸ë©˜í…Œì´ì…˜]")
en = "Welcome. First topic is AI. It changes everything. Moving on to cloud computing. It's growing fast. Let's talk about security."
segs = segment_topics(en)
print(f"  ì„¸ê·¸ë¨¼íŠ¸: {len(segs)}")
for s in segs:
    print(f"    [{s['segment']}] {s['text'][:60]}")

# Korean segmenter
print("\n[í•œêµ­ì–´ ì„¸ê·¸ë©˜í…Œì´ì…˜]")
ko = "ì•ˆë…•í•˜ì„¸ìš”. ì²« ë²ˆì§¸ ì£¼ì œëŠ” ê²½ì œì…ë‹ˆë‹¤. ì„±ì¥ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì ë‹¤ìŒìœ¼ë¡œ ë¶€ë™ì‚° ì–˜ê¸°ë¥¼ í•˜ê² ìŠµë‹ˆë‹¤. ì§‘ê°’ì´ ì˜¬ëìŠµë‹ˆë‹¤. ì ë§ˆì§€ë§‰ ì£¼ì œëŠ” AIì…ë‹ˆë‹¤."
segs_ko = segment_topics(ko)
print(f"  ì„¸ê·¸ë¨¼íŠ¸: {len(segs_ko)}")
for s in segs_ko:
    print(f"    [{s['segment']}] {s['text'][:60]}")

# Sentiment
print("\n[ê°ì„± ë¶„ì„ í…ŒìŠ¤íŠ¸]")
sent_tests = [
    ("ì •ë§ ìµœê³ ì˜ˆìš”! ê°ì‚¬í•©ë‹ˆë‹¤", "positive"),
    ("ìµœì•…ì´ë‹¤ ì‹¤ë§", "negative"),
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤", "neutral"),
    ("This is amazing and wonderful", "positive"),
    ("terrible and boring", "negative"),
]
correct = 0
for text, expected in sent_tests:
    got = _analyze_sentiment(text)
    ok = got == expected
    correct += ok
    print(f"  {'âœ…' if ok else 'âŒ'} '{text}' â†’ {got} (expected {expected})")
print(f"  ê°ì„± ì •í™•ë„: {correct}/{len(sent_tests)} = {correct/len(sent_tests)*100:.0f}%")

# Noise filter
print("\n[ë…¸ì´ì¦ˆ í•„í„° í…ŒìŠ¤íŠ¸]")
noise_tests = [
    ("ã…‹ã…‹", True), ("ğŸ˜‚ğŸ˜‚ğŸ˜‚", True), ("êµ¬ë… ì¢‹ì•„ìš” ëˆŒëŸ¬ì£¼ì„¸ìš”", True),
    ("sub 4 sub check my channel", True), ("ì´ ì˜ìƒ ì •ë§ ìœ ìµí•´ìš” ê²½ì œì— ëŒ€í•´ ë§ì´ ë°°ì› ìŠµë‹ˆë‹¤", False),
    ("hi", True), ("Great video explaining complex topics clearly", False),
]
nc = 0
for text, expected in noise_tests:
    got = _is_noise(text)
    ok = got == expected
    nc += ok
    print(f"  {'âœ…' if ok else 'âŒ'} '{text[:40]}' noise={got} (expected {expected})")
print(f"  ë…¸ì´ì¦ˆ í•„í„° ì •í™•ë„: {nc}/{len(noise_tests)} = {nc/len(noise_tests)*100:.0f}%")

print("\nâœ… ê²€ì¦ ì™„ë£Œ")
