"""TextRank vs TF-IDF extractive summary ë¹„êµ í…ŒìŠ¤íŠ¸"""
import re
import math
from collections import Counter

# â”€â”€ ê³µí†µ ìœ í‹¸ â”€â”€

_MUSIC_RE = re.compile(r"\[?[â™ªâ™«â™¬]+\]?")
_STOPWORDS = frozenset(
    "the a an is are was were be been being have has had do does did will would "
    "shall should may might can could of in to for on with at by from as into "
    "through during before after above below between and but or nor not so yet "
    "this that these those it its he she they we you i me him her us them my "
    "his our your their what which who whom how when where why all each every "
    "both few more most other some such no any if than too very just about also "
    "then only still even because since while although though after before until "
    "ì€ ëŠ” ì´ ê°€ ì„ ë¥¼ ì— ì—ì„œ ì˜ ì™€ ê³¼ ë„ ë¡œ ìœ¼ë¡œ í•œ ëœ í•˜ëŠ” ìˆëŠ” ì—†ëŠ” ê·¸ ì´ ì € ê²ƒ ìˆ˜ ë“± "
    "ì¢€ ì˜ ë” ë˜ ì•ˆ ëª» ì œ ë„ˆ ë‚˜ ìš” ë„¤ ê±° ê±´ ê²Œ ë° ë•Œ ê³³ ì¤‘ ë‹¤ í•´ ì¤˜ ì¤„ ê±¸ ë­ ì™œ".split()
)

_IMPORTANCE_RE = re.compile(
    r"\b(ê²°ë¡ |í•µì‹¬|ìš”ì•½í•˜ë©´|ì •ë¦¬í•˜ë©´|ìš”ì |ì¤‘ìš”í•œ|"
    r"in summary|to summarize|the key point|importantly|in conclusion|"
    r"takeaway|bottom line|to recap|the main|crucial|essential)\b",
    re.IGNORECASE,
)

def clean(text):
    text = _MUSIC_RE.sub("", text)
    return re.sub(r"\s{2,}", " ", text).strip()

def split_sentences(text):
    # ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ + ê³µë°±, ë˜ëŠ” ì¤„ë°”ê¿ˆ
    parts = re.split(r"(?<=[.!?ã€‚])\s+|\n+", text)
    return [p.strip() for p in parts if p and len(p.strip()) > 15]

def tokenize(text):
    return [w.lower() for w in re.findall(r"[a-zA-Zê°€-í£\d]+", text) if len(w) > 1]

def adaptive_max_chars(text_len):
    if text_len < 1000: return max(200, int(text_len * 0.50))
    if text_len < 5000: return max(200, int(text_len * 0.20))
    if text_len < 20000: return max(200, int(text_len * 0.10))
    return max(200, int(text_len * 0.05))


# â”€â”€ TextRank êµ¬í˜„ â”€â”€

def _cosine_similarity(a_tokens, b_tokens):
    """ë‘ ë¬¸ì¥ì˜ í† í° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
    a_filtered = [t for t in a_tokens if t not in _STOPWORDS]
    b_filtered = [t for t in b_tokens if t not in _STOPWORDS]
    if not a_filtered or not b_filtered:
        return 0.0
    a_counter = Counter(a_filtered)
    b_counter = Counter(b_filtered)
    all_words = set(a_counter) | set(b_counter)
    dot = sum(a_counter.get(w, 0) * b_counter.get(w, 0) for w in all_words)
    mag_a = math.sqrt(sum(v**2 for v in a_counter.values()))
    mag_b = math.sqrt(sum(v**2 for v in b_counter.values()))
    if mag_a == 0 or mag_b == 0:
        return 0.0
    return dot / (mag_a * mag_b)

def textrank_summary(text, max_sentences=7, max_chars=0):
    """TextRank ê¸°ë°˜ ìš”ì•½: ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ê·¸ë˜í”„ â†’ PageRank â†’ ìƒìœ„ ë¬¸ì¥ ì„ íƒ"""
    text = clean(text)
    if not text:
        return ""
    if max_chars <= 0:
        max_chars = adaptive_max_chars(len(text))
    
    sentences = split_sentences(text)
    sentences = [s for s in sentences if len(s.strip()) > 20]
    if len(sentences) <= max_sentences:
        result = ". ".join(sentences)
        return result[:max_chars]
    
    n = len(sentences)
    
    # 1. í† í°í™”
    sent_tokens = [tokenize(s) for s in sentences]
    
    # 2. ìœ ì‚¬ë„ í–‰ë ¬ êµ¬ì¶•
    similarity_matrix = [[0.0] * n for _ in range(n)]
    for i in range(n):
        for j in range(i + 1, n):
            sim = _cosine_similarity(sent_tokens[i], sent_tokens[j])
            similarity_matrix[i][j] = sim
            similarity_matrix[j][i] = sim
    
    # 3. PageRank ë°˜ë³µ (damping=0.85, 30íšŒ)
    damping = 0.85
    scores = [1.0 / n] * n
    for _ in range(30):
        new_scores = [0.0] * n
        for i in range(n):
            rank_sum = 0.0
            for j in range(n):
                if i == j:
                    continue
                # jì—ì„œ ë‚˜ê°€ëŠ” ë§í¬ì˜ í•©
                out_sum = sum(similarity_matrix[j][k] for k in range(n) if k != j)
                if out_sum > 0:
                    rank_sum += similarity_matrix[j][i] * scores[j] / out_sum
            new_scores[i] = (1 - damping) / n + damping * rank_sum
        scores = new_scores
    
    # 4. ìœ„ì¹˜ + í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
    for i in range(n):
        if i == 0:
            scores[i] *= 1.3
        elif i == n - 1:
            scores[i] *= 1.2
        if _IMPORTANCE_RE.search(sentences[i]):
            scores[i] *= 1.5
    
    # 5. ìƒìœ„ ë¬¸ì¥ ì„ íƒ (ì¤‘ë³µ ì œê±°)
    ranked = sorted(range(n), key=lambda i: scores[i], reverse=True)
    selected_indices = []
    for idx in ranked:
        if len(selected_indices) >= max_sentences:
            break
        # ì¤‘ë³µ ì²´í¬ (Jaccard > 0.5)
        is_dup = False
        for sel_idx in selected_indices:
            ta = set(sent_tokens[idx]) - _STOPWORDS
            tb = set(sent_tokens[sel_idx]) - _STOPWORDS
            if ta and tb and len(ta & tb) / len(ta | tb) > 0.5:
                is_dup = True
                break
        if not is_dup:
            selected_indices.append(idx)
    
    # 6. ì›ë˜ ìˆœì„œë¡œ ì •ë ¬
    selected_indices.sort()
    
    # 7. max_chars ë‚´ì—ì„œ ì¡°í•©
    parts = []
    total = 0
    for idx in selected_indices:
        s = sentences[idx]
        addition = len(s) + (2 if parts else 0)
        if total + addition > max_chars and parts:
            break
        parts.append(s)
        total += addition
    
    result = ". ".join(parts)
    if result and not result.endswith((".", "!", "?")):
        result += "."
    return result


# â”€â”€ ê¸°ì¡´ TF-IDF (í˜„ì¬ ì½”ë“œ ê·¸ëŒ€ë¡œ) â”€â”€

def tfidf_summary(text, max_sentences=7, max_chars=0):
    """í˜„ì¬ extractive_summaryì™€ ë™ì¼í•œ ë¡œì§"""
    from mcp_youtube_intelligence.core.summarizer import extractive_summary
    return extractive_summary(text, max_sentences=max_sentences, max_chars=max_chars)


# â”€â”€ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ â”€â”€

if __name__ == "__main__":
    from mcp_youtube_intelligence.core.transcript import fetch_transcript
    
    videos = [
        ("kCc8FmEb1nY", "Let's build GPT (Karpathy, ì˜ì–´ ê¸´ ì˜ìƒ)"),
        ("pBy1zgt0XPc", "í•œêµ­ì–´ ì§§ì€ ì˜ìƒ"),
        ("aircAruvnKk", "í•œêµ­ì–´ ì¤‘ê°„ ì˜ìƒ"),
    ]
    
    for vid, desc in videos:
        print(f"\n{'='*80}")
        print(f"ğŸ“¹ {desc} ({vid})")
        print(f"{'='*80}")
        
        result = fetch_transcript(vid)
        raw = result['best']
        if not raw:
            print("  âŒ ìë§‰ ì—†ìŒ, ìŠ¤í‚µ")
            continue
        
        print(f"  ğŸ“ ì›ë³¸: {len(raw):,} chars | ì–¸ì–´: {result['lang']}")
        print(f"  ğŸ“ ëª©í‘œ ìš”ì•½ ê¸¸ì´: {adaptive_max_chars(len(raw))} chars")
        
        # TF-IDF ìš”ì•½
        tfidf = tfidf_summary(raw)
        print(f"\n  â”€â”€ TF-IDF (í˜„ì¬) â”€â”€")
        print(f"  ê¸¸ì´: {len(tfidf)} chars ({len(tfidf)*100//len(raw)}% of original)")
        print(f"  ë‚´ìš©: {tfidf[:400]}")
        
        # TextRank ìš”ì•½  
        tr = textrank_summary(raw)
        print(f"\n  â”€â”€ TextRank (ì‹ ê·œ) â”€â”€")
        print(f"  ê¸¸ì´: {len(tr)} chars ({len(tr)*100//len(raw)}% of original)")
        print(f"  ë‚´ìš©: {tr[:400]}")
        
        # ë¹„êµ ë¶„ì„
        print(f"\n  â”€â”€ ë¹„êµ â”€â”€")
        # ê²¹ì¹˜ëŠ” ë¬¸ì¥ í™•ì¸
        tfidf_sents = set(tfidf.split(". "))
        tr_sents = set(tr.split(". "))
        overlap = tfidf_sents & tr_sents
        print(f"  ê²¹ì¹˜ëŠ” ë¬¸ì¥: {len(overlap)}/{max(len(tfidf_sents), len(tr_sents))}")
        print(f"  TF-IDF ê³ ìœ  ë¬¸ì¥ ìˆ˜: {len(tfidf_sents - tr_sents)}")
        print(f"  TextRank ê³ ìœ  ë¬¸ì¥ ìˆ˜: {len(tr_sents - tfidf_sents)}")
